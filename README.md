# Kernel In-Context Learning

This repository investigates whether Transformers can learn kernelized functions in-context, with a focus on RBF kernels approximated using Random Fourier Features (RFF).

## ğŸ“Š Research Question

Can transformers implicitly approximate kernelized models such as RBF-kernel ridge regression purely through in-context learning, without explicit kernel feature computation?

This investigation builds upon the framework from [Garg et al. (2022)](https://arxiv.org/pdf/2208.01066), extending their function class from linear to kernelized (non-linear) functions.

## ğŸ§© Key Components

- **Random Fourier Features (RFF)**: Used to approximate RBF kernels, bridging classical kernel theory with modern transformer models
- **In-context Learning Transformer**: A transformer model that learns to predict function values from context examples
- **Kernel Ridge Regression**: Serves as a baseline for comparison

## ğŸ› ï¸ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/kernel-icl.git
cd kernel-icl

# Install dependencies
pip install -r requirements.txt
```

## ğŸš€ Usage

```bash
# Train with default settings
python src/train.py

# Train with a specific configuration
python src/train.py --config configs/default.yaml
```

## ğŸ“ Project Structure

```
kernel-icl/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py         # Main training script
â”‚   â”œâ”€â”€ model.py         # Transformer model implementations
â”‚   â”œâ”€â”€ kernels.py       # RFF implementations and kernel methods
â”‚   â”œâ”€â”€ data.py          # Dataset creation utilities
â”‚   â”œâ”€â”€ eval.py          # Evaluation utilities
â”‚   â””â”€â”€ config.py        # Configuration management
â”œâ”€â”€ configs/             # Configuration files
â”‚   â””â”€â”€ default.yaml     # Default configuration
â””â”€â”€ outputs/             # Training outputs (created during training)
    â””â”€â”€ YYYYMMDD_HHMMSS/ # Timestamped experiment directories
```

## ğŸ“ Methodology

Our approach:

1. **Generate RFF Functions**: We use Random Fourier Features to approximate RBF kernels
2. **In-Context Learning**: The transformer learns to predict function values based on context examples
3. **Evaluation**: We compare transformer performance with kernel ridge regression baselines

## ğŸ“ˆ Results

The key metrics we analyze:

- MSE between transformer predictions and true function values
- Comparison with kernel ridge regression at various regularization strengths
- How performance varies with kernel parameters (e.g., Ïƒ)

## ğŸ” Parameter Selection

- **RFF dimension**: Controls the approximation quality of the kernel
- **Sigma**: Controls the smoothness of the generated functions
- **Context length**: Number of examples provided for in-context learning
- **Input dimension**: Dimension of the input space

## ğŸ“š Related Work

- [In-context Learning and Induction Heads](https://arxiv.org/pdf/2208.01066) (Garg et al., 2022)
- [Transformers Learn In-Context by Gradient Descent](https://arxiv.org/abs/2212.07677) (Hahn et al., 2022)

## ğŸ¤ Contributing

Contributions welcome! Please feel free to submit a Pull Request.